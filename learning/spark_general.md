### General ideas of apache spark
* It tries to address the challenges of big data.
* It's based on the parallel processing of data in the _memory_ of a cluster of machines. This is opposite to MapReduce/Hadoop which uses hard drives for IO.
* Spark hides the implementation details.
